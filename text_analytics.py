# -*- coding: utf-8 -*-
"""Text Analytics

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O92Jc3xxP487c1HSOfhXFJEzQ0-A26Jy

# Text Analytics:
"""

# imports 
import json
import gzip

import nltk
nltk.download('punkt')

from nltk.tokenize import sent_tokenize, word_tokenize

from google.colab import drive
drive.mount('/content/drive')

def read_gz_file(filename:str):
    l = []
    with gzip.open(filename, 'r') as f:
        for jsonObj in f:
            data = json.loads(jsonObj)
            l.append(data)


    return l
file_ = read_gz_file("/content/drive/MyDrive/Colab Notebooks/Assignment/Cell_Phones_and_Accessories_5.json.gz")

file_

type(file_)

file_[1]

"""# Sentence segmentation. How many sentences are segmented by an exclamation mark, question mark, and period?

"""

corpus = []

for i in range(len(file_)):
    try:
        corpus.append(str(file_[i]['reviewText']))
    
    except KeyError:
        pass

len(corpus)

corpus[2]

sent = []
for s in corpus:
    sent.extend(sent_tokenize(s))

def sentenceSegmentation(tokenized_corpus):

  exclamation = 0
  question = 0
  period = 0

  for i in range(len(tokenized_corpus)):
    for j in range(len(tokenized_corpus[i])):
      if tokenized_corpus[i][j][-1] == ".":
        period += 1
      elif tokenized_corpus[i][j][-1] == "?":
        question += 1
      elif tokenized_corpus[i][j][-1] == "!":
        exclamation += 1

  return(exclamation , question , period)

exclamation, question, period = sentenceSegmentation(sent)

print(f"Total number of sentences ending with ! is {exclamation} \n")
print(f"Total number of sentences ending with ? is {question} \n")
print(f"Total number of sentences ending with . is {period} \n")

"""> ### Data Preprocessing 


For data preprocessing of this corpus I'd would following preprocessing steps



1.   Removing punctuations like . , ! $( ) * % @
2.   Removing URLs
3.   Removing Stop words
4.   Lower casing
5.   Tokenization
6.   Contractions








"""

pip install contractions

nltk.download("stopwords")

import re
import contractions
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

import string 

string.punctuation

def preprocessing(sentence):

  punct = r"[!\"#\$%&\'\(\)\*\+,-\./:;<=>\?@\[\\\]\^_`{\|}~]"
  tokenize = RegexpTokenizer(r'\w+')
  
  sentence = re.sub(punct, "", sentence)
  sentence = contractions.fix(sentence)
  sentence = re.sub(r'https?', "", sentence) 
  sentence = re.sub(r'[0-9]', '', sentence)
  sentence = sentence.lower()
  sentence = tokenize.tokenize(sentence)

  return(sentence)

pip install tqdm

"""# Finding frequency and the rank of tokens"""

import tqdm

from tqdm import tqdm

vocab = {}


for i in tqdm(range(len(sent))):

  stop_words = set(stopwords.words("english"))
  sentence  = preprocessing(sent[i])

  for word in sentence:
    if word in stop_words:
      pass

    if word in vocab.keys():
      vocab[word] +=1
    else:
      vocab[word] = 1

len(vocab)

sorted_vocab = dict(sorted(vocab.items(), key=lambda x:x[1], reverse=True))

total_num_words = sum(sorted_vocab.values())

total_num_words

freq = list(sorted_vocab.values())

count_less = 0
word_less = 0

count_more = 0
word_more = 0
for i in range(len(freq)):
  if freq[i]<10:
    count_less += freq[i] 
    word_less += 1
  
  elif freq[i]>100000:
    count_more += freq[i]
    word_more += 1

word_less/len(vocab)

word_less

word_more/len(vocab)

word_more

"""A total of 87.5% words are useless for our analysis as we are rejecting words having frequency more than 100000 and less than 10. 

Hence, we can say that Pareto rule is applicable here.

# Sentiment analysis using Stanza library and Naive Bayes
"""

pip install stanza

import stanza
stanza.download('en')
stanza_pipeline = stanza.Pipeline(lang = 'en', processors='tokenize, sentiment', tokenize_no_ssplit=True)

stanza_Value = stanza_pipeline(sent[1])

def sentiment_val(sentence):
  stanza_Value = stanza_pipeline(sentence)
  sentiment = stanza_Value.sentences[0].sentiment

  if sentiment ==0 :
    return(True , 0)
  if sentiment == 2:
    return(True, 1)
  else:
    return(False, 0)

import random

def sampleGenerator(corpus , idx , num_samples):
  
  sample_X = []
  sample_Y = []
  total = 0

  while total < num_samples:
    # generate index
    index = int(random.random()*10000)

    if index not in idx:
      sample = corpus[index]
      idx.append(index)

    not_neutral, sentiment = sentiment_val(sample)

    if sentiment_val(sample)[0]:
      total += 1
      sample_X.append(sample)
      sample_Y.append(sentiment)

  return(idx , sample_X , sample_Y)

a, sentiment = sentiment_val(sent[1])

sentiment

sampleGenerator(sent , [0] , 10)

ind_train , train_X , train_Y =sampleGenerator(sent , [0] , 500)

ind_test , test_X , test_Y =sampleGenerator(sent , ind_train , 250)

len(train_X) , len(train_Y)

len(test_X) , len(test_Y)

from sklearn import feature_extraction
extractor = feature_extraction.text.CountVectorizer(stop_words='english')
X_train = extractor.fit_transform([x for x in train_X])
X_test = extractor.transform([x for x in test_X])

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X_train, train_Y)

from sklearn import metrics
import matplotlib.pyplot as plt
plt.style.use('classic')
print(metrics.plot_confusion_matrix(model, X_test,test_Y,display_labels=['0','1']))

y_predict = model.predict(X_test)
print(f"Accuracy = {metrics.accuracy_score(test_Y,y_predict)}")
print(f"Precision = {metrics.precision_score(test_Y,y_predict)}")
print(f"Recall =  {metrics.recall_score(test_Y,y_predict)}")
print(f"F1 Score = {metrics.f1_score(test_Y,y_predict)}")

"""### End of Assignment 3

# Identifying aspects and sentiments using POS rules
"""

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from collections import Counter

def Validate_chunking( corpus , aspect_rule):
	aspect_dict = Counter()
	opinion_dict = Counter()
	intensifier_dict = Counter()
	
	lemma = WordNetLemmatizer()

	i = 1

	for sentence in corpus:
		tokens = nltk.word_tokenize(sentence.lower())
		lemmatized_tokens = [lemma.lemmatize(token) for token in tokens]
		tags = nltk.pos_tag(lemmatized_tokens)
		
		aspect =  aspect_rule
		
		cp = nltk.RegexpParser(aspect)
		res = cp.parse(tags)
		for x in res:
			if type(x) == nltk.tree.Tree:
				for y in x.leaves():
					if 'NN' in y[1]: 
						aspect = [y[0]]
						aspect_dict.update(aspect)

					elif 'JJ' in y[1]:
						opinion = [y[0]]
						opinion_dict.update(opinion)
				
					elif 'RB' in y[1]: 
						intensifier = [y[0]]
						intensifier_dict.update(intensifier)
				
				
			print(i,end=' sentences processed \r')
			i += 1

	print("Aspects: ", aspect_dict.most_common(15))
	print("Opinions: ", opinion_dict.most_common(15))
	print("Intensifier: ", intensifier_dict.most_common(15))
	
	return (aspect_dict, opinion_dict, intensifier_dict)

rule = ["{<JJ><NN.*>+}","{<NN.*>+}", "{<VB.*><NN.*>}", "{<NN.*><VB.*>}", "{<NN><NNS>}", "{<NNS> <JJ>}", "{<NNS><VB.*>}", "{<NN.*><JJ>}", "{<NN.*><VBZ>}", "{<RB.*><JJ>+}","{<RB><VBG>}"]

rule_new  = [ "r """ "ASPECT:" "" + rule  for rule in rule]

rule_new

sample_data = random.sample(sent , 1000)

"""### Validating rule 1 
$ \{<JJ> <NN.*>+\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[0]))

"""### Validating rule 2
$ \{<NN.*>+\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[1]))

"""### Validating rule 3
$ \{<VB.*><NN.*>\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[2]))

"""### Validating rule 4
$ \{<NN.*><VB.*>\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[3]))

"""### Validating rule 5
$ \{<NN><NNS>\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[4]))

"""### Validating rule 6
$ \{{NNS><JJ>}\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[5]))

"""### Validating rule 7
$ \{ <NNS><VB.*>\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[6]))

"""### Validating rule 8
$ \{\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[7]))

"""### Validating rule 9
$ \{\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[8]))

"""### Validating rule 10
$ \{\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[9]))

"""### Validating rule 11
$ \{\} $  
"""

aspect , opinion , intensifier = Validate_chunking( sample_data ,  str(rule_new[10]))

"""#### Now let's test the same on 1 percent of corpus """

aspects = []
opinions = []
intensifiers = []

sample_data_new = random.sample(sent , int(len(sent)*0.01))

for i in range(11):
  aspect , opinion , intensifier = Validate_chunking( sample_data_new ,  str(rule_new[i]))
  if len(aspect) == 0:
    aspects.append(False)
  else:
    aspects.append(True)
  
  if len(opinion) == 0:
    opinions.append(False)
  else:
    opinions.append(True)

  if len(intensifier) == 0:
    intensifiers.append(False)
  else:
    intensifiers.append(True)

aspects, opinions, intensifiers

"""### Result"""

import pandas as pd
d = {"Rule" : rule , "Aspect":aspects , "Opinion": opinions, "Intensifier": intensifiers }

pd.DataFrame(data=d)

{<JJ.*><NN.*>}
        {<NN.*><JJ.*>}

        {<NN.*>+}

        {<RB.*><JJ.*>}
        {<RB.*><VB.*>}

"""Apart form these rules few other rules can be:

Aspect
- $\{<RB.?>*<VB.?>*<NNP>+<NN>?\}$

Opinion
-  $\{<JJ. *><NN. *>\}$ 

Intensifier 
- $\{<RB. *><JJ. *>} {<RB. *><VB. *>\}

- \{<RBR. *><JJ. *>} {<RBR. *><VB. *>\}

- \{<RBS. *><JJ. *>} {<RBS. *><VB. *>\}$

"""

def chunkRule( corpus , rule):
	aspect_dict = Counter()
	opinion_dict = Counter()
	intensifier_dict = Counter()
	
	lemma = WordNetLemmatizer()

	i = 1

	for sentence in corpus:
		tokens = nltk.word_tokenize(sentence.lower())
		lemmatized_tokens = [lemma.lemmatize(token) for token in tokens]
		tags = nltk.pos_tag(lemmatized_tokens)
		
		
		cp = nltk.RegexpParser(rule)
		res = cp.parse(tags)
		for x in res:
			if type(x) == nltk.tree.Tree:
				for y in x.leaves():
					if 'NN' in y[1]: 
						aspect = [y[0]]
						aspect_dict.update(aspect)

					elif 'JJ' in y[1]:
						opinion = [y[0]]
						opinion_dict.update(opinion)
				
					elif 'RB' in y[1]: 
						intensifier = [y[0]]
						intensifier_dict.update(intensifier)
				
				
			print(i,end=' sentences processed \r')
			i += 1

	print("Top_15_Aspects: ", aspect_dict.most_common(15))
	print("Top_15_Opinions: ", opinion_dict.most_common(15))
	print("Top_15_Intensifier: ", intensifier_dict.most_common(15))
	
	return (aspect_dict, opinion_dict, intensifier_dict)

"""Let's run this on same 1 percent data"""

rule  = r""" ASPECT: 
                  {<RB.?>*<VB.?>*<NNP>+<NN>?}
                  {<JJ.*><NN.*>} 
                  {<NN.*>+} 
                  {<RBS.*><JJ.*>} 
                  {<RBS.*><VB.*>} 
                  {<RBR.*><JJ.*>} 
                  {<RBR.*><VB.*>} 
                  {<RB.*><JJ.*>} 
                  {<RB.*><VB.*>}"""

aspect, opinion, intensifier = chunkRule( sample_data_new , str(rule))